{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11813145,"sourceType":"datasetVersion","datasetId":7419647}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:33.715481Z","iopub.execute_input":"2025-05-15T15:22:33.716012Z","iopub.status.idle":"2025-05-15T15:22:36.835159Z","shell.execute_reply.started":"2025-05-15T15:22:33.715985Z","shell.execute_reply":"2025-05-15T15:22:36.834336Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"wandb.login(key=\"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:36.836040Z","iopub.execute_input":"2025-05-15T15:22:36.836506Z","iopub.status.idle":"2025-05-15T15:22:42.940976Z","shell.execute_reply.started":"2025-05-15T15:22:36.836479Z","shell.execute_reply":"2025-05-15T15:22:42.940232Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23s025\u001b[0m (\u001b[33mcs23s025-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:42.941834Z","iopub.execute_input":"2025-05-15T15:22:42.942359Z","iopub.status.idle":"2025-05-15T15:22:42.963894Z","shell.execute_reply.started":"2025-05-15T15:22:42.942330Z","shell.execute_reply":"2025-05-15T15:22:42.963201Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# File paths\ntrain_csv = \"/kaggle/input/dakshina-dataset-hindi/DakshinaDataSet_Hindi/hindi_Train_dataset.csv\"\ntest_csv = \"/kaggle/input/dakshina-dataset-hindi/DakshinaDataSet_Hindi/hindi_Test_dataset.csv\"\nval_csv = \"/kaggle/input/dakshina-dataset-hindi/DakshinaDataSet_Hindi/hindi_Validation_dataset.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:42.964789Z","iopub.execute_input":"2025-05-15T15:22:42.965067Z","iopub.status.idle":"2025-05-15T15:22:42.980310Z","shell.execute_reply.started":"2025-05-15T15:22:42.965047Z","shell.execute_reply":"2025-05-15T15:22:42.979607Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_data = pd.read_csv(train_csv, header=None)\ntrain_input = train_data[0].to_numpy()\ntrain_output = train_data[1].to_numpy()\nval_data = pd.read_csv(val_csv, header=None)\nval_input = val_data[0].to_numpy()\nval_output = val_data[1].to_numpy()\ntest_data = pd.read_csv(test_csv, header=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:42.981014Z","iopub.execute_input":"2025-05-15T15:22:42.981279Z","iopub.status.idle":"2025-05-15T15:22:43.064387Z","shell.execute_reply.started":"2025-05-15T15:22:42.981251Z","shell.execute_reply":"2025-05-15T15:22:43.063492Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def pre_processing(train_input, train_output):\n    data = {\n        \"all_characters\": [],\n        \"char_num_map\": {},\n        \"num_char_map\": {},\n        \"source_charToNum\": torch.zeros(len(train_input), 30, dtype=torch.int, device=device),\n        \"source_data\": train_input,\n        \"all_characters_2\": [],\n        \"char_num_map_2\": {},\n        \"num_char_map_2\": {},\n        \"val_charToNum\": torch.zeros(len(train_output), 23, dtype=torch.int, device=device),\n        \"target_data\": train_output,\n        \"source_len\": 0,\n        \"target_len\": 0\n    }\n    k = 0\n    for i in range(len(train_input)):\n        train_input[i] = \"{\" + train_input[i] + \"}\" * (29 - len(train_input[i]))\n        charToNum = []\n        for char in train_input[i]:\n            if char not in data[\"all_characters\"]:\n                data[\"all_characters\"].append(char)\n                index = data[\"all_characters\"].index(char)\n                data[\"char_num_map\"][char] = index\n                data[\"num_char_map\"][index] = char\n            else:\n                index = data[\"all_characters\"].index(char)\n            charToNum.append(index)\n        my_tensor = torch.tensor(charToNum, device=device)\n        data[\"source_charToNum\"][k] = my_tensor\n\n        charToNum1 = []\n        train_output[i] = \"{\" + train_output[i] + \"}\" * (22 - len(train_output[i]))\n        for char in train_output[i]:\n            if char not in data[\"all_characters_2\"]:\n                data[\"all_characters_2\"].append(char)\n                index = data[\"all_characters_2\"].index(char)\n                data[\"char_num_map_2\"][char] = index\n                data[\"num_char_map_2\"][index] = char\n            else:\n                index = data[\"all_characters_2\"].index(char)\n            charToNum1.append(index)\n        my_tensor1 = torch.tensor(charToNum1, device=device)\n        data[\"val_charToNum\"][k] = my_tensor1\n        k += 1\n\n    data[\"source_len\"] = len(data[\"all_characters\"])\n    data[\"target_len\"] = len(data[\"all_characters_2\"])\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:43.067902Z","iopub.execute_input":"2025-05-15T15:22:43.068136Z","iopub.status.idle":"2025-05-15T15:22:43.076504Z","shell.execute_reply.started":"2025-05-15T15:22:43.068118Z","shell.execute_reply":"2025-05-15T15:22:43.075685Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"data = pre_processing(copy.copy(train_input), copy.copy(train_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:43.077098Z","iopub.execute_input":"2025-05-15T15:22:43.077301Z","iopub.status.idle":"2025-05-15T15:22:47.665764Z","shell.execute_reply.started":"2025-05-15T15:22:43.077286Z","shell.execute_reply":"2025-05-15T15:22:47.665081Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def pre_processing_validation(val_input, val_output):\n    data2 = {\n        \"all_characters\": [],\n        \"char_num_map\": {},\n        \"num_char_map\": {},\n        \"source_charToNum\": torch.zeros(len(val_input), 30, dtype=torch.int, device=device),\n        \"source_data\": val_input,\n        \"all_characters_2\": [],\n        \"char_num_map_2\": {},\n        \"num_char_map_2\": {},\n        \"val_charToNum\": torch.zeros(len(val_output), 23, dtype=torch.int, device=device),\n        \"target_data\": val_output,\n        \"source_len\": 0,\n        \"target_len\": 0\n    }\n    k = 0\n    m1 = data[\"char_num_map\"]\n    m2 = data[\"char_num_map_2\"]\n    for i in range(len(val_input)):\n        val_input[i] = \"{\" + val_input[i] + \"}\" * (29 - len(val_input[i]))\n        charToNum = []\n        for char in val_input[i]:\n            if char not in data2[\"all_characters\"]:\n                data2[\"all_characters\"].append(char)\n                index = m1[char]\n                data2[\"char_num_map\"][char] = index\n                data2[\"num_char_map\"][index] = char\n            else:\n                index = m1[char]\n            charToNum.append(index)\n        my_tensor = torch.tensor(charToNum, device=device)\n        data2[\"source_charToNum\"][k] = my_tensor\n\n        charToNum1 = []\n        val_output[i] = \"{\" + val_output[i] + \"}\" * (22 - len(val_output[i]))\n        for char in val_output[i]:\n            if char not in data2[\"all_characters_2\"]:\n                data2[\"all_characters_2\"].append(char)\n                index = m2[char]\n                data2[\"char_num_map_2\"][char] = index\n                data2[\"num_char_map_2\"][index] = char\n            else:\n                index = m2[char]\n            charToNum1.append(index)\n        my_tensor1 = torch.tensor(charToNum1, device=device)\n        data2[\"val_charToNum\"][k] = my_tensor1\n        k += 1\n\n    data2[\"source_len\"] = len(data2[\"all_characters\"])\n    data2[\"target_len\"] = len(data2[\"all_characters_2\"])\n    return data2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:47.666865Z","iopub.execute_input":"2025-05-15T15:22:47.667120Z","iopub.status.idle":"2025-05-15T15:22:47.675303Z","shell.execute_reply.started":"2025-05-15T15:22:47.667095Z","shell.execute_reply":"2025-05-15T15:22:47.674535Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"data2 = pre_processing_validation(copy.copy(val_input), copy.copy(val_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:47.676119Z","iopub.execute_input":"2025-05-15T15:22:47.676375Z","iopub.status.idle":"2025-05-15T15:22:48.110335Z","shell.execute_reply.started":"2025-05-15T15:22:47.676358Z","shell.execute_reply":"2025-05-15T15:22:48.109556Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, x, y):\n        self.source = x\n        self.target = y\n\n    def __len__(self):\n        return len(self.source)\n\n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data\n\ndef validationAccuracy(encoder, decoder, batchsize, tf_ratio, cellType, bidirection):\n    dataLoader = dataLoaderFun(\"validation\", batchsize)\n    encoder.eval()\n    decoder.eval()\n    total_sequences = 0\n    total_correct_sequences = 0\n    total_char_matches = 0\n    total_characters = 0\n    total_loss = 0\n\n    lossFunction = nn.NLLLoss()\n\n    for source_batch, target_batch in dataLoader:\n        actual_batch_size = source_batch.shape[0]\n        total_sequences += actual_batch_size\n        total_characters += target_batch.numel()\n\n        encoder_initial_state = encoder.getInitialState(actual_batch_size)\n        if bidirection == \"Yes\":\n            reversed_batch = torch.flip(source_batch, dims=[1])\n            source_batch = (source_batch + reversed_batch) // 2\n        if cellType == 'LSTM':\n            encoder_initial_state = (encoder_initial_state, encoder.getInitialState(actual_batch_size))\n\n        encoder_states, _ = encoder(source_batch, encoder_initial_state)\n        decoder_current_state = encoder_states[-1, :, :, :]\n        encoder_final_layer_states = encoder_states[:, -1, :, :]\n        output_seq_len = target_batch.shape[1]\n\n        loss = 0\n        decoder_actual_output = []\n        randNumber = random.random()\n\n        for i in range(output_seq_len):\n            if i == 0:\n                decoder_current_input = torch.full((actual_batch_size, 1), 0, device=device)\n            else:\n                if randNumber < tf_ratio:\n                    decoder_current_input = target_batch[:, i].reshape(actual_batch_size, 1)\n                else:\n                    decoder_current_input = decoder_current_input.reshape(actual_batch_size, 1)\n            decoder_output, decoder_current_state, _ = decoder(decoder_current_input, decoder_current_state, encoder_final_layer_states)\n            topv, topi = decoder_output.topk(1)\n            decoder_current_input = topi.squeeze().detach()\n            decoder_actual_output.append(decoder_current_input)\n\n            decoder_output = decoder_output[:, -1, :]\n            curr_target_chars = target_batch[:, i].long()\n            loss += lossFunction(decoder_output, curr_target_chars)\n\n        total_loss += loss.item() / output_seq_len\n        decoder_actual_output = torch.cat(decoder_actual_output, dim=0).reshape(output_seq_len, actual_batch_size).transpose(0, 1)\n        total_correct_sequences += (decoder_actual_output == target_batch).all(dim=1).sum().item()\n        total_char_matches += (decoder_actual_output == target_batch).sum().item()\n\n    encoder.train()\n    decoder.train()\n\n    wandb.log({\n        'validation_loss': total_loss / len(dataLoader),\n        'validation_accuracy': total_correct_sequences / total_sequences,\n        'validation_char_accuracy': total_char_matches / total_characters\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:48.111321Z","iopub.execute_input":"2025-05-15T15:22:48.111568Z","iopub.status.idle":"2025-05-15T15:22:48.121945Z","shell.execute_reply.started":"2025-05-15T15:22:48.111546Z","shell.execute_reply":"2025-05-15T15:22:48.121136Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = scores.squeeze().unsqueeze(1)\n        weights = F.softmax(scores, dim=0)\n        weights = weights.permute(2, 1, 0)\n        keys = keys.permute(1, 0, 2)\n        context = torch.bmm(weights, keys)\n        return context, weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:48.122740Z","iopub.execute_input":"2025-05-15T15:22:48.123082Z","iopub.status.idle":"2025-05-15T15:22:48.139787Z","shell.execute_reply.started":"2025-05-15T15:22:48.123063Z","shell.execute_reply":"2025-05-15T15:22:48.139062Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, inputDim, embSize, encoderLayers, hiddenLayerNuerons, cellType, batch_size):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(inputDim, embSize)\n        self.encoderLayers = encoderLayers\n        self.hiddenLayerNuerons = hiddenLayerNuerons\n        self.cellType = cellType\n        if cellType == 'GRU':\n            self.rnn = nn.GRU(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n        elif cellType == 'RNN':\n            self.rnn = nn.RNN(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n        else:\n            self.rnn = nn.LSTM(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n\n    def forward(self, sourceBatch, encoderCurrState):\n        sequenceLength = sourceBatch.shape[1]\n        batch_size = sourceBatch.shape[0]\n        encoderStates = torch.zeros(sequenceLength, self.encoderLayers, batch_size, self.hiddenLayerNuerons, device=device)\n        for i in range(sequenceLength):\n            currInput = sourceBatch[:, i].reshape(batch_size, 1)\n            _, encoderCurrState = self.statesCalculation(currInput, encoderCurrState)\n            if self.cellType == 'LSTM':\n                encoderStates[i] = encoderCurrState[1]\n            else:\n                encoderStates[i] = encoderCurrState\n        return encoderStates, encoderCurrState\n\n    def statesCalculation(self, currentInput, prevState):\n        embdInput = self.embedding(currentInput)\n        output, prev_state = self.rnn(embdInput, prevState)\n        return output, prev_state\n\n    def getInitialState(self, batch_size):\n        return torch.zeros(self.encoderLayers, batch_size, self.hiddenLayerNuerons, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:48.140422Z","iopub.execute_input":"2025-05-15T15:22:48.140608Z","iopub.status.idle":"2025-05-15T15:22:48.158270Z","shell.execute_reply.started":"2025-05-15T15:22:48.140593Z","shell.execute_reply":"2025-05-15T15:22:48.157673Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, outputDim, embSize, hiddenLayerNuerons, decoderLayers, cellType, dropout_p):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(outputDim, embSize)\n        self.cellType = cellType\n        if cellType == 'GRU':\n            self.rnn = nn.GRU(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        elif cellType == 'RNN':\n            self.rnn = nn.RNN(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        else:\n            self.rnn = nn.LSTM(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        self.fc = nn.Linear(hiddenLayerNuerons, outputDim)\n        self.softmax = nn.LogSoftmax(dim=2)\n        self.dropout = nn.Dropout(dropout_p)\n        self.attention = Attention(hiddenLayerNuerons).to(device)\n\n    def forward(self, current_input, prev_state, encoder_final_layers):\n        if self.cellType == 'LSTM':\n            context, attn_weights = self.attention(prev_state[1][-1, :, :], encoder_final_layers)\n        else:\n            context, attn_weights = self.attention(prev_state[-1, :, :], encoder_final_layers)\n        embd_input = self.embedding(current_input)\n        curr_embd = F.relu(embd_input)\n        input_gru = torch.cat((curr_embd, context), dim=2)\n        output, prev_state = self.rnn(input_gru, prev_state)\n        output = self.dropout(output)\n        output = self.softmax(self.fc(output))\n        return output, prev_state, attn_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:48.158961Z","iopub.execute_input":"2025-05-15T15:22:48.159121Z","iopub.status.idle":"2025-05-15T15:22:48.179403Z","shell.execute_reply.started":"2025-05-15T15:22:48.159109Z","shell.execute_reply":"2025-05-15T15:22:48.178661Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def dataLoaderFun(dataName, batch_size):\n    if dataName == 'train':\n        dataset = MyDataset(data[\"source_charToNum\"], data['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    else:\n        dataset = MyDataset(data2[\"source_charToNum\"], data2['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ndef train(embSize, encoderLayers, decoderLayers, hiddenLayerNuerons, cellType, bidirection, dropout, epochs, batchsize, learningRate, optimizer, tf_ratio):\n    dataLoader = dataLoaderFun(\"train\", batchsize)\n    encoder = Encoder(data[\"source_len\"], embSize, encoderLayers, hiddenLayerNuerons, cellType, batchsize).to(device)\n    decoder = Decoder(data[\"target_len\"], embSize, hiddenLayerNuerons, encoderLayers, cellType, dropout).to(device)\n    if optimizer == 'Adam':\n        encoderOptimizer = optim.Adam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.Adam(decoder.parameters(), lr=learningRate)\n    else:\n        encoderOptimizer = optim.NAdam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.NAdam(decoder.parameters(), lr=learningRate)\n    lossFunction = nn.NLLLoss()\n    for epoch in range(epochs):\n        train_accuracy = 0\n        train_loss = 0\n        for batch_num, (source_batch, target_batch) in enumerate(dataLoader):\n            actual_batch_size = source_batch.shape[0]\n            encoder_initial_state = encoder.getInitialState(actual_batch_size)\n            if bidirection == \"Yes\":\n                reversed_batch = torch.flip(source_batch, dims=[1])\n                source_batch = (source_batch + reversed_batch) // 2\n            if cellType == 'LSTM':\n                encoder_initial_state = (encoder_initial_state, encoder.getInitialState(actual_batch_size))\n            encoder_states, dummy = encoder(source_batch, encoder_initial_state)\n            decoder_current_state = dummy\n            encoder_final_layer_states = encoder_states[:, -1, :, :]\n            loss = 0\n            output_seq_len = target_batch.shape[1]\n            decoder_actual_output = []\n            randNumber = random.random()\n            for i in range(output_seq_len):\n                if i == 0:\n                    decoder_current_input = torch.full((actual_batch_size, 1), 0, device=device)\n                else:\n                    if randNumber < tf_ratio:\n                        decoder_current_input = target_batch[:, i].reshape(actual_batch_size, 1)\n                    else:\n                        decoder_current_input = decoder_current_input.reshape(actual_batch_size, 1)\n                decoder_output, decoder_current_state, _ = decoder(decoder_current_input, decoder_current_state, encoder_final_layer_states)\n                topv, topi = decoder_output.topk(1)\n                decoder_current_input = topi.squeeze().detach()\n                decoder_actual_output.append(decoder_current_input)\n                decoder_output = decoder_output[:, -1, :]\n                curr_target_chars = target_batch[:, i].type(dtype=torch.long)\n                loss += (lossFunction(decoder_output, curr_target_chars))\n            decoder_actual_output = torch.cat(decoder_actual_output, dim=0).reshape(output_seq_len, actual_batch_size).transpose(0, 1)\n            train_accuracy += (decoder_actual_output == target_batch).all(dim=1).sum().item()\n            train_loss += (loss.item() / output_seq_len)\n            encoderOptimizer.zero_grad()\n            decoderOptimizer.zero_grad()\n            loss.backward()\n            encoderOptimizer.step()\n            decoderOptimizer.step()\n\n        #Logging train metrics here\n        wandb.log({'train_accuracy': train_accuracy / len(data[\"source_charToNum\"])})\n        wandb.log({'train_loss': train_loss / len(dataLoader)})\n\n        validationAccuracy(encoder, decoder, batchsize, tf_ratio, cellType, bidirection)\n\n\ndef numToCharConverter(inputArray, outputArray, data):\n    mp = data['num_char_map_2']\n    for row1, row2 in zip(inputArray, outputArray):\n        t1 = ''.join([mp[e1.item()] for e1 in row1])\n        t2 = ''.join([mp[e2.item()] for e2 in row2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:48.180216Z","iopub.execute_input":"2025-05-15T15:22:48.180875Z","iopub.status.idle":"2025-05-15T15:22:48.199447Z","shell.execute_reply.started":"2025-05-15T15:22:48.180849Z","shell.execute_reply":"2025-05-15T15:22:48.198725Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\ndef train_model():\n    # Initialize wandb run first\n    with wandb.init(project='CS23S025-Assignment-3-DL') as run:\n        config = wandb.config\n\n        # Dynamically name the run after initialization\n        run.name = f\"embedding{config.embSize}_cellType{config.cellType}_batchSize{config.batchsize}\"\n        # Call your training logic\n        train(\n             embSize=config.embSize,\n            encoderLayers=config.encoderLayers,\n            decoderLayers=config.decoderLayers,\n            hiddenLayerNuerons=config.hiddenLayerNuerons,\n            cellType=config.cellType,\n            bidirection=config.bidirection,\n            dropout=config.dropout,\n            epochs=config.epochs,\n            batchsize=config.batchsize,\n            learningRate=config.learningRate,\n            optimizer=config.optimizer,\n            tf_ratio=config.tf_ratio\n        )\n\n# Define sweep configuration\nsweep_config = {\n    'method': 'bayes',\n    'name': 'Assignment_3_withAttention_2',\n    'metric': {\n        'goal': 'maximize',\n        'name': 'validation_accuracy',\n    },\n    'parameters': {\n        'embSize': {'values': [16, 32, 64,128,256]},\n        'encoderLayers': {'values': [1, 5, 10]},\n        'decoderLayers': {'values': [1, 5, 10,12,15]},\n        'hiddenLayerNuerons': {'values': [64, 256, 512]},\n        'cellType': {'values': ['GRU', 'RNN']},\n        'bidirection': {'values': ['no', 'Yes']},\n        'dropout': {'values': [0, 0.1,0.2,0.5]},\n        'epochs': {'values': [15,20,25,30]},\n        'batchsize': {'values': [32, 64,128]},\n        'learningRate': {'values': [1e-2, 1e-3, 1e-4]},\n        'optimizer': {'values': ['Adam', 'Nadam']},\n        'tf_ratio': {'values': [0.2, 0.4, 0.5,0.7]}\n    }\n}\n\n# Create the sweep\nsweep_id = wandb.sweep(sweep=sweep_config, project='CS23S025-Assignment-3-DL')\n\n# Launch the sweep agent (make sure to update the entity if needed)\nwandb.agent(\n    sweep_id=sweep_id,\n    function=train_model,\n    count=50,  # or however many runs you want\n    entity=\"cs23s025-indian-institute-of-technology-madras\",\n    project=\"CS23S025-Assignment-3-DL\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:22:48.200308Z","iopub.execute_input":"2025-05-15T15:22:48.200548Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: 7tu6j1w6\nSweep URL: https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL/sweeps/7tu6j1w6\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uaccmo55 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchsize: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirection: Yes\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcellType: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoderLayers: 12\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembSize: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoderLayers: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n\u001b[34m\u001b[1mwandb\u001b[0m: \thiddenLayerNuerons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttf_ratio: 0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'CS23S025-Assignment-3-DL' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250515_152255-uaccmo55</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL/runs/uaccmo55' target=\"_blank\">comfy-sweep-1</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL/sweeps/7tu6j1w6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL/sweeps/7tu6j1w6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL/sweeps/7tu6j1w6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL/sweeps/7tu6j1w6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL/runs/uaccmo55' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-3-DL/runs/uaccmo55</a>"},"metadata":{}}],"execution_count":null}]}