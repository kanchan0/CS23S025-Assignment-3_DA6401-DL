{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:40.783826Z",
     "iopub.status.busy": "2025-05-20T16:12:40.783582Z",
     "iopub.status.idle": "2025-05-20T16:12:48.547264Z",
     "shell.execute_reply": "2025-05-20T16:12:48.546642Z",
     "shell.execute_reply.started": "2025-05-20T16:12:40.783785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_MODE=disabled\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import wandb\n",
    "%env WANDB_MODE = disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:48.549214Z",
     "iopub.status.busy": "2025-05-20T16:12:48.548864Z",
     "iopub.status.idle": "2025-05-20T16:12:54.458614Z",
     "shell.execute_reply": "2025-05-20T16:12:54.457877Z",
     "shell.execute_reply.started": "2025-05-20T16:12:48.549197Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:54.460396Z",
     "iopub.status.busy": "2025-05-20T16:12:54.459439Z",
     "iopub.status.idle": "2025-05-20T16:12:54.518462Z",
     "shell.execute_reply": "2025-05-20T16:12:54.517778Z",
     "shell.execute_reply.started": "2025-05-20T16:12:54.460365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:54.519380Z",
     "iopub.status.busy": "2025-05-20T16:12:54.519187Z",
     "iopub.status.idle": "2025-05-20T16:12:54.538580Z",
     "shell.execute_reply": "2025-05-20T16:12:54.537799Z",
     "shell.execute_reply.started": "2025-05-20T16:12:54.519364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_csv = \"/kaggle/input/dakshina-dataset-hindi/DakshinaDataSet_Hindi/hindi_Train_dataset.csv\"\n",
    "test_csv = \"/kaggle/input/dakshina-dataset-hindi/DakshinaDataSet_Hindi/hindi_Test_dataset.csv\"\n",
    "val_csv = \"/kaggle/input/dakshina-dataset-hindi/DakshinaDataSet_Hindi/hindi_Validation_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:54.539684Z",
     "iopub.status.busy": "2025-05-20T16:12:54.539418Z",
     "iopub.status.idle": "2025-05-20T16:12:54.677617Z",
     "shell.execute_reply": "2025-05-20T16:12:54.677044Z",
     "shell.execute_reply.started": "2025-05-20T16:12:54.539661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_csv, header=None)\n",
    "train_input = train_data[0].to_numpy()\n",
    "train_output = train_data[1].to_numpy()\n",
    "val_data = pd.read_csv(val_csv, header=None)\n",
    "val_input = val_data[0].to_numpy()\n",
    "val_output = val_data[1].to_numpy()\n",
    "test_data = pd.read_csv(test_csv, header=None)\n",
    "test_input = test_data[0].to_numpy()\n",
    "test_output = test_data[1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:54.678602Z",
     "iopub.status.busy": "2025-05-20T16:12:54.678357Z",
     "iopub.status.idle": "2025-05-20T16:12:54.687739Z",
     "shell.execute_reply": "2025-05-20T16:12:54.687064Z",
     "shell.execute_reply.started": "2025-05-20T16:12:54.678585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pre_processing(train_input, train_output):\n",
    "    data = {\n",
    "        \"all_characters\": [],\n",
    "        \"char_num_map\": {},\n",
    "        \"num_char_map\": {},\n",
    "        \"source_charToNum\": torch.zeros(len(train_input), 30, dtype=torch.int, device=device),\n",
    "        \"source_data\": train_input,\n",
    "        \"all_characters_2\": [],\n",
    "        \"char_num_map_2\": {},\n",
    "        \"num_char_map_2\": {},\n",
    "        \"val_charToNum\": torch.zeros(len(train_output), 23, dtype=torch.int, device=device),\n",
    "        \"target_data\": train_output,\n",
    "        \"source_len\": 0,\n",
    "        \"target_len\": 0\n",
    "    }\n",
    "    k = 0\n",
    "    for i in range(len(train_input)):\n",
    "        train_input[i] = \"{\" + train_input[i] + \"}\" * (29 - len(train_input[i]))\n",
    "        charToNum = []\n",
    "        for char in train_input[i]:\n",
    "            if char not in data[\"all_characters\"]:\n",
    "                data[\"all_characters\"].append(char)\n",
    "                index = data[\"all_characters\"].index(char)\n",
    "                data[\"char_num_map\"][char] = index\n",
    "                data[\"num_char_map\"][index] = char\n",
    "            else:\n",
    "                index = data[\"all_characters\"].index(char)\n",
    "            charToNum.append(index)\n",
    "        my_tensor = torch.tensor(charToNum, device=device)\n",
    "        data[\"source_charToNum\"][k] = my_tensor\n",
    "\n",
    "        charToNum1 = []\n",
    "        train_output[i] = \"{\" + train_output[i] + \"}\" * (22 - len(train_output[i]))\n",
    "        for char in train_output[i]:\n",
    "            if char not in data[\"all_characters_2\"]:\n",
    "                data[\"all_characters_2\"].append(char)\n",
    "                index = data[\"all_characters_2\"].index(char)\n",
    "                data[\"char_num_map_2\"][char] = index\n",
    "                data[\"num_char_map_2\"][index] = char\n",
    "            else:\n",
    "                index = data[\"all_characters_2\"].index(char)\n",
    "            charToNum1.append(index)\n",
    "        my_tensor1 = torch.tensor(charToNum1, device=device)\n",
    "        data[\"val_charToNum\"][k] = my_tensor1\n",
    "        k += 1\n",
    "\n",
    "    data[\"source_len\"] = len(data[\"all_characters\"])\n",
    "    data[\"target_len\"] = len(data[\"all_characters_2\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:54.690214Z",
     "iopub.status.busy": "2025-05-20T16:12:54.690013Z",
     "iopub.status.idle": "2025-05-20T16:12:59.492595Z",
     "shell.execute_reply": "2025-05-20T16:12:59.491860Z",
     "shell.execute_reply.started": "2025-05-20T16:12:54.690199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pre_processing(copy.copy(train_input), copy.copy(train_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:59.493572Z",
     "iopub.status.busy": "2025-05-20T16:12:59.493306Z",
     "iopub.status.idle": "2025-05-20T16:12:59.501859Z",
     "shell.execute_reply": "2025-05-20T16:12:59.501033Z",
     "shell.execute_reply.started": "2025-05-20T16:12:59.493548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pre_processing_validation(val_input, val_output):\n",
    "    data2 = {\n",
    "        \"all_characters\": [],\n",
    "        \"char_num_map\": {},\n",
    "        \"num_char_map\": {},\n",
    "        \"source_charToNum\": torch.zeros(len(val_input), 30, dtype=torch.int, device=device),\n",
    "        \"source_data\": val_input,\n",
    "        \"all_characters_2\": [],\n",
    "        \"char_num_map_2\": {},\n",
    "        \"num_char_map_2\": {},\n",
    "        \"val_charToNum\": torch.zeros(len(val_output), 23, dtype=torch.int, device=device),\n",
    "        \"target_data\": val_output,\n",
    "        \"source_len\": 0,\n",
    "        \"target_len\": 0\n",
    "    }\n",
    "    k = 0\n",
    "    m1 = data[\"char_num_map\"]\n",
    "    m2 = data[\"char_num_map_2\"]\n",
    "    for i in range(len(val_input)):\n",
    "        val_input[i] = \"{\" + val_input[i] + \"}\" * (29 - len(val_input[i]))\n",
    "        charToNum = []\n",
    "        for char in val_input[i]:\n",
    "            if char not in data2[\"all_characters\"]:\n",
    "                data2[\"all_characters\"].append(char)\n",
    "                index = m1[char]\n",
    "                data2[\"char_num_map\"][char] = index\n",
    "                data2[\"num_char_map\"][index] = char\n",
    "            else:\n",
    "                index = m1[char]\n",
    "            charToNum.append(index)\n",
    "        my_tensor = torch.tensor(charToNum, device=device)\n",
    "        data2[\"source_charToNum\"][k] = my_tensor\n",
    "\n",
    "        charToNum1 = []\n",
    "        val_output[i] = \"{\" + val_output[i] + \"}\" * (22 - len(val_output[i]))\n",
    "        for char in val_output[i]:\n",
    "            if char not in data2[\"all_characters_2\"]:\n",
    "                data2[\"all_characters_2\"].append(char)\n",
    "                index = m2[char]\n",
    "                data2[\"char_num_map_2\"][char] = index\n",
    "                data2[\"num_char_map_2\"][index] = char\n",
    "            else:\n",
    "                index = m2[char]\n",
    "            charToNum1.append(index)\n",
    "        my_tensor1 = torch.tensor(charToNum1, device=device)\n",
    "        data2[\"val_charToNum\"][k] = my_tensor1\n",
    "        k += 1\n",
    "\n",
    "    data2[\"source_len\"] = len(data2[\"all_characters\"])\n",
    "    data2[\"target_len\"] = len(data2[\"all_characters_2\"])\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:12:59.502932Z",
     "iopub.status.busy": "2025-05-20T16:12:59.502637Z",
     "iopub.status.idle": "2025-05-20T16:13:00.438697Z",
     "shell.execute_reply": "2025-05-20T16:13:00.438120Z",
     "shell.execute_reply.started": "2025-05-20T16:12:59.502908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data2 = pre_processing_validation(copy.copy(val_input), copy.copy(val_output))\n",
    "data_test = pre_processing_validation(copy.copy(test_input), copy.copy(test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.439716Z",
     "iopub.status.busy": "2025-05-20T16:13:00.439452Z",
     "iopub.status.idle": "2025-05-20T16:13:00.444561Z",
     "shell.execute_reply": "2025-05-20T16:13:00.443731Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.439691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['all_characters', 'char_num_map', 'num_char_map', 'source_charToNum', 'source_data', 'all_characters_2', 'char_num_map_2', 'num_char_map_2', 'val_charToNum', 'target_data', 'source_len', 'target_len'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.445577Z",
     "iopub.status.busy": "2025-05-20T16:13:00.445332Z",
     "iopub.status.idle": "2025-05-20T16:13:00.463255Z",
     "shell.execute_reply": "2025-05-20T16:13:00.462517Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.445555Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.source = x\n",
    "        self.target = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_data = self.source[idx]\n",
    "        target_data = self.target[idx]\n",
    "        return source_data, target_data\n",
    "\n",
    "def validationAccuracy(encoder, decoder, batchsize, tf_ratio, cellType, bidirection):\n",
    "    dataLoader = dataLoaderFun(\"validation\", batchsize)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_sequences = 0\n",
    "    total_correct_sequences = 0\n",
    "    total_char_matches = 0\n",
    "    total_characters = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    lossFunction = nn.NLLLoss()\n",
    "\n",
    "    for source_batch, target_batch in dataLoader:\n",
    "        actual_batch_size = source_batch.shape[0]\n",
    "        total_sequences += actual_batch_size\n",
    "        total_characters += target_batch.numel()\n",
    "\n",
    "        encoder_initial_state = encoder.getInitialState(actual_batch_size)\n",
    "        if bidirection == \"Yes\":\n",
    "            reversed_batch = torch.flip(source_batch, dims=[1])\n",
    "            source_batch = (source_batch + reversed_batch) // 2\n",
    "        if cellType == 'LSTM':\n",
    "            encoder_initial_state = (encoder_initial_state, encoder.getInitialState(actual_batch_size))\n",
    "\n",
    "        encoder_states, _ = encoder(source_batch, encoder_initial_state)\n",
    "        decoder_current_state = encoder_states[-1, :, :, :]\n",
    "        encoder_final_layer_states = encoder_states[:, -1, :, :]\n",
    "        output_seq_len = target_batch.shape[1]\n",
    "\n",
    "        loss = 0\n",
    "        decoder_actual_output = []\n",
    "        randNumber = random.random()\n",
    "\n",
    "        for i in range(output_seq_len):\n",
    "            if i == 0:\n",
    "                decoder_current_input = torch.full((actual_batch_size, 1), 0, device=device)\n",
    "            else:\n",
    "                if randNumber < tf_ratio:\n",
    "                    decoder_current_input = target_batch[:, i].reshape(actual_batch_size, 1)\n",
    "                else:\n",
    "                    decoder_current_input = decoder_current_input.reshape(actual_batch_size, 1)\n",
    "            decoder_output, decoder_current_state, _ = decoder(decoder_current_input, decoder_current_state, encoder_final_layer_states)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_current_input = topi.squeeze().detach()\n",
    "            decoder_actual_output.append(decoder_current_input)\n",
    "\n",
    "            decoder_output = decoder_output[:, -1, :]\n",
    "            curr_target_chars = target_batch[:, i].long()\n",
    "            loss += lossFunction(decoder_output, curr_target_chars)\n",
    "\n",
    "        total_loss += loss.item() / output_seq_len\n",
    "        decoder_actual_output = torch.cat(decoder_actual_output, dim=0).reshape(output_seq_len, actual_batch_size).transpose(0, 1)\n",
    "        total_correct_sequences += (decoder_actual_output == target_batch).all(dim=1).sum().item()\n",
    "        total_char_matches += (decoder_actual_output == target_batch).sum().item()\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    wandb.log({\n",
    "        'validation_loss': total_loss / len(dataLoader),\n",
    "        'validation_accuracy': total_correct_sequences / total_sequences,\n",
    "        'validation_char_accuracy': total_char_matches / total_characters\n",
    "    })\n",
    "    return total_correct_sequences / total_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.464621Z",
     "iopub.status.busy": "2025-05-20T16:13:00.464023Z",
     "iopub.status.idle": "2025-05-20T16:13:00.483097Z",
     "shell.execute_reply": "2025-05-20T16:13:00.482336Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.464596Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "#     def forward(self, query, keys):\n",
    "#         scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "#         scores = scores.squeeze().unsqueeze(1)\n",
    "#         weights = F.softmax(scores, dim=0)\n",
    "#         weights = weights.permute(2, 1, 0)\n",
    "#         keys = keys.permute(1, 0, 2)\n",
    "#         context = torch.bmm(weights, keys)\n",
    "#         return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.484114Z",
     "iopub.status.busy": "2025-05-20T16:13:00.483845Z",
     "iopub.status.idle": "2025-05-20T16:13:00.499508Z",
     "shell.execute_reply": "2025-05-20T16:13:00.498796Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.484091Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, inputDim, embSize, encoderLayers, hiddenLayerNuerons, cellType, batch_size):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.embedding = nn.Embedding(inputDim, embSize)\n",
    "#         self.encoderLayers = encoderLayers\n",
    "#         self.hiddenLayerNuerons = hiddenLayerNuerons\n",
    "#         self.cellType = cellType\n",
    "#         self.num_directions = 2 if bidirection == \"Yes\" else 1\n",
    "        \n",
    "#         if cellType == 'GRU':\n",
    "#             self.rnn = nn.GRU(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n",
    "#         elif cellType == 'RNN':\n",
    "#             self.rnn = nn.RNN(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n",
    "#         else:\n",
    "#             self.rnn = nn.LSTM(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n",
    "\n",
    "#     def forward(self, sourceBatch, encoderCurrState):\n",
    "#         sequenceLength = sourceBatch.shape[1]\n",
    "#         batch_size = sourceBatch.shape[0]\n",
    "#         encoderStates = torch.zeros(sequenceLength, self.encoderLayers, batch_size, self.hiddenLayerNuerons, device=device)\n",
    "#         for i in range(sequenceLength):\n",
    "#             currInput = sourceBatch[:, i].reshape(batch_size, 1)\n",
    "#             _, encoderCurrState = self.statesCalculation(currInput, encoderCurrState)\n",
    "#             if self.cellType == 'LSTM':\n",
    "#                 encoderStates[i] = encoderCurrState[1]\n",
    "#             else:\n",
    "#                 encoderStates[i] = encoderCurrState\n",
    "#         return encoderStates, encoderCurrState\n",
    "        \n",
    "#     def initHidden(self, batch_size=1):\n",
    "#         h0 = torch.zeros(self.encoderLayers * self.num_directions,\n",
    "#                          batch_size,\n",
    "#                          self.hiddenLayerNuerons,\n",
    "#                          device=device)\n",
    "#         if isinstance(self.rnn, nn.LSTM):\n",
    "#             c0 = torch.zeros_like(h0)\n",
    "#             return (h0, c0)\n",
    "#         else:\n",
    "#             return h0\n",
    "#     def statesCalculation(self, currentInput, prevState):\n",
    "#         embdInput = self.embedding(currentInput)\n",
    "#         output, prev_state = self.rnn(embdInput, prevState)\n",
    "#         return output, prev_state\n",
    "\n",
    "#     def getInitialState(self, batch_size):\n",
    "#         return torch.zeros(self.encoderLayers, batch_size, self.hiddenLayerNuerons, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.500513Z",
     "iopub.status.busy": "2025-05-20T16:13:00.500265Z",
     "iopub.status.idle": "2025-05-20T16:13:00.517262Z",
     "shell.execute_reply": "2025-05-20T16:13:00.516419Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.500495Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, outputDim, embSize, hiddenLayerNuerons, decoderLayers, cellType, dropout_p):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.embedding = nn.Embedding(outputDim, embSize)\n",
    "#         self.cellType = cellType\n",
    "#         if cellType == 'GRU':\n",
    "#             self.rnn = nn.GRU(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n",
    "#         elif cellType == 'RNN':\n",
    "#             self.rnn = nn.RNN(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n",
    "#         else:\n",
    "#             self.rnn = nn.LSTM(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hiddenLayerNuerons, outputDim)\n",
    "#         self.softmax = nn.LogSoftmax(dim=2)\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "#         self.attention = Attention(hiddenLayerNuerons).to(device)\n",
    "\n",
    "#     def forward(self, current_input, prev_state, encoder_final_layers):\n",
    "#         if self.cellType == 'LSTM':\n",
    "#             context, attn_weights = self.attention(prev_state[1][-1, :, :], encoder_final_layers)\n",
    "#         else:\n",
    "#             context, attn_weights = self.attention(prev_state[-1, :, :], encoder_final_layers)\n",
    "#         embd_input = self.embedding(current_input)\n",
    "#         curr_embd = F.relu(embd_input)\n",
    "#         input_gru = torch.cat((curr_embd, context), dim=2)\n",
    "#         output, prev_state = self.rnn(input_gru, prev_state)\n",
    "#         output = self.dropout(output)\n",
    "#         output = self.softmax(self.fc(output))\n",
    "#         return output, prev_state, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.518397Z",
     "iopub.status.busy": "2025-05-20T16:13:00.518072Z",
     "iopub.status.idle": "2025-05-20T16:13:00.534961Z",
     "shell.execute_reply": "2025-05-20T16:13:00.534314Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.518371Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#From ChatGPT\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        \"\"\"\n",
    "        query: (batch, hidden_size)\n",
    "        keys:  (seq_len, batch, hidden_size)\n",
    "        returns:\n",
    "            context: (batch, 1, hidden_size)\n",
    "            weights: (batch, 1, seq_len)\n",
    "        \"\"\"\n",
    "        # reshape keys to (batch, seq_len, hidden_size)\n",
    "        keys_trans = keys.permute(1, 0, 2)            # (batch, seq_len, hidden)\n",
    "\n",
    "        # expand query to (batch, seq_len, hidden_size)\n",
    "        query_expanded = query.unsqueeze(1).repeat(1, keys_trans.size(1), 1)\n",
    "\n",
    "        # compute score with additive attention\n",
    "        scores = self.Va(torch.tanh(self.Wa(query_expanded) + self.Ua(keys_trans)))  # (batch, seq_len, 1)\n",
    "        scores = scores.squeeze(2).unsqueeze(1)    # (batch, 1, seq_len)\n",
    "\n",
    "        # normalize to obtain attention weights\n",
    "        weights = F.softmax(scores, dim=2)         # (batch, 1, seq_len)\n",
    "\n",
    "        # compute context vector as weighted sum\n",
    "        context = torch.bmm(weights, keys_trans)   # (batch, 1, hidden)\n",
    "        return context, weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inputDim: int,\n",
    "                 embSize: int,\n",
    "                 encoderLayers: int,\n",
    "                 hiddenLayerNuerons: int,\n",
    "                 cellType: str = 'LSTM',\n",
    "                 bidirectional: bool = False,\n",
    "                 dropout: float = 0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(inputDim, embSize)\n",
    "        self.cell_type = cellType.upper()\n",
    "        self.num_layers = encoderLayers\n",
    "        self.hidden_size = hiddenLayerNuerons\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        rnn_kwargs = {\n",
    "            'input_size': embSize,\n",
    "            'hidden_size': hiddenLayerNuerons,\n",
    "            'num_layers': encoderLayers,\n",
    "            'batch_first': True,\n",
    "            'dropout': dropout if encoderLayers > 1 else 0.0,\n",
    "            'bidirectional': bidirectional\n",
    "        }\n",
    "        if self.cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(**rnn_kwargs)\n",
    "        elif self.cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(**rnn_kwargs)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(**rnn_kwargs)\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        # src: (batch, seq_len)\n",
    "        emb = self.embedding(src)                     # (batch, seq_len, emb_size)\n",
    "        outputs, hidden = self.rnn(emb, hidden)       # outputs: (batch, seq_len, hidden*directions)\n",
    "\n",
    "        # prepare encoder states for attention\n",
    "        # convert to (seq_len, batch, hidden*directions)\n",
    "        encoder_states = outputs.permute(1, 0, 2)     # (seq_len, batch, hidden*directions)\n",
    "        return encoder_states, hidden\n",
    "\n",
    "    def initHidden(self, batch_size: int, device: torch.device):\n",
    "        num = self.num_layers * self.num_directions\n",
    "        h0 = torch.zeros(num, batch_size, self.hidden_size, device=device)\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            return (h0, c0)\n",
    "        return h0\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 outputDim: int,\n",
    "                 embSize: int,\n",
    "                 hiddenLayerNuerons: int,\n",
    "                 decoderLayers: int,\n",
    "                 cellType: str = 'LSTM',\n",
    "                 dropout_p: float = 0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(outputDim, embSize)\n",
    "        self.cell_type = cellType.upper()\n",
    "        self.hidden_size = hiddenLayerNuerons\n",
    "        self.num_layers = decoderLayers\n",
    "\n",
    "        rnn_kwargs = {\n",
    "            'input_size': embSize + hiddenLayerNuerons,\n",
    "            'hidden_size': hiddenLayerNuerons,\n",
    "            'num_layers': decoderLayers,\n",
    "            'batch_first': True,\n",
    "            'dropout': dropout_p if decoderLayers > 1 else 0.0\n",
    "        }\n",
    "        if self.cell_type == 'GRU':\n",
    "            self.rnn = nn.GRU(**rnn_kwargs)\n",
    "        elif self.cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(**rnn_kwargs)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(**rnn_kwargs)\n",
    "\n",
    "        self.attention = Attention(hiddenLayerNuerons)\n",
    "        self.fc = nn.Linear(hiddenLayerNuerons, outputDim)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input_step, hidden, encoder_states):\n",
    "        # input_step: (batch, 1)\n",
    "        # hidden:    (h_n, c_n) or h_n\n",
    "        # encoder_states: (seq_len, batch, hidden)\n",
    "\n",
    "        # get context from attention\n",
    "        if isinstance(hidden, tuple):  # LSTM\n",
    "            query = hidden[0][-1]  # take last layer's hidden state (batch, hidden)\n",
    "        else:\n",
    "            query = hidden[-1]     # (batch, hidden)\n",
    "        context, attn_weights = self.attention(query, encoder_states)\n",
    "\n",
    "        # embed input and concat with context\n",
    "        emb = self.embedding(input_step)              # (batch, 1, emb_size)\n",
    "        emb = F.relu(emb)\n",
    "        rnn_input = torch.cat((emb, context), dim=2)  # (batch, 1, emb_size+hidden)\n",
    "\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        output = self.dropout(output)                 # (batch, 1, hidden)\n",
    "        prediction = self.softmax(self.fc(output))  # (batch, 1, output_dim)\n",
    "        return prediction, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.535779Z",
     "iopub.status.busy": "2025-05-20T16:13:00.535557Z",
     "iopub.status.idle": "2025-05-20T16:13:00.553608Z",
     "shell.execute_reply": "2025-05-20T16:13:00.552868Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.535764Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dataLoaderFun(dataName, batch_size):\n",
    "    if dataName == 'train':\n",
    "        dataset = MyDataset(data[\"source_charToNum\"], data['val_charToNum'])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        dataset = MyDataset(data2[\"source_charToNum\"], data2['val_charToNum'])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def train(embSize, encoderLayers, decoderLayers, hiddenLayerNuerons, cellType, bidirection, dropout, epochs, batchsize, learningRate, optimizer, tf_ratio):\n",
    "    dataLoader = dataLoaderFun(\"train\", batchsize)\n",
    "    encoder = Encoder(data[\"source_len\"], embSize, encoderLayers, hiddenLayerNuerons, cellType, batchsize).to(device)\n",
    "    decoder = Decoder(data[\"target_len\"], embSize, hiddenLayerNuerons,decoderLayers, cellType, dropout).to(device)\n",
    "    if optimizer == 'Adam':\n",
    "        encoderOptimizer = optim.Adam(encoder.parameters(), lr=learningRate)\n",
    "        decoderOptimizer = optim.Adam(decoder.parameters(), lr=learningRate)\n",
    "    else:\n",
    "        encoderOptimizer = optim.NAdam(encoder.parameters(), lr=learningRate)\n",
    "        decoderOptimizer = optim.NAdam(decoder.parameters(), lr=learningRate)\n",
    "    lossFunction = nn.NLLLoss()\n",
    "    best_val_acc =0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        for batch_num, (source_batch, target_batch) in enumerate(dataLoader):\n",
    "            actual_batch_size = source_batch.shape[0]\n",
    "            encoder_initial_state = encoder.getInitialState(actual_batch_size)\n",
    "            if bidirection == \"Yes\":\n",
    "                reversed_batch = torch.flip(source_batch, dims=[1])\n",
    "                source_batch = (source_batch + reversed_batch) // 2\n",
    "            if cellType == 'LSTM':\n",
    "                encoder_initial_state = (encoder_initial_state, encoder.getInitialState(actual_batch_size))\n",
    "            encoder_states, dummy = encoder(source_batch, encoder_initial_state)\n",
    "            decoder_current_state = dummy\n",
    "            encoder_final_layer_states = encoder_states[:, -1, :, :]\n",
    "            loss = 0\n",
    "            output_seq_len = target_batch.shape[1]\n",
    "            decoder_actual_output = []\n",
    "            randNumber = random.random()\n",
    "            for i in range(output_seq_len):\n",
    "                if i == 0:\n",
    "                    decoder_current_input = torch.full((actual_batch_size, 1), 0, device=device)\n",
    "                else:\n",
    "                    if randNumber < tf_ratio:\n",
    "                        decoder_current_input = target_batch[:, i].reshape(actual_batch_size, 1)\n",
    "                    else:\n",
    "                        decoder_current_input = decoder_current_input.reshape(actual_batch_size, 1)\n",
    "                decoder_output, decoder_current_state, _ = decoder(decoder_current_input, decoder_current_state, encoder_final_layer_states)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_current_input = topi.squeeze().detach()\n",
    "                decoder_actual_output.append(decoder_current_input)\n",
    "                decoder_output = decoder_output[:, -1, :]\n",
    "                curr_target_chars = target_batch[:, i].type(dtype=torch.long)\n",
    "                loss += (lossFunction(decoder_output, curr_target_chars))\n",
    "            decoder_actual_output = torch.cat(decoder_actual_output, dim=0).reshape(output_seq_len, actual_batch_size).transpose(0, 1)\n",
    "            train_accuracy += (decoder_actual_output == target_batch).all(dim=1).sum().item()\n",
    "            train_loss += (loss.item() / output_seq_len)\n",
    "            encoderOptimizer.zero_grad()\n",
    "            decoderOptimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            encoderOptimizer.step()\n",
    "            decoderOptimizer.step()\n",
    "        \n",
    "        #Logging train metrics here\n",
    "        wandb.log({'train_accuracy': train_accuracy / len(data[\"source_charToNum\"])})\n",
    "        wandb.log({'train_loss': train_loss / len(dataLoader)})\n",
    "\n",
    "        val_acc = validationAccuracy(encoder, decoder, batchsize, tf_ratio, cellType, bidirection)\n",
    "        if( val_acc > best_val_acc ):\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                }, \"best_model.pth\")\n",
    "            print(checkpoint['decoder_state_dict'].keys())\n",
    "            print(f\"New best model saved with accuracy: { best_val_acc:.4f}\")\n",
    "\n",
    "def numToCharConverter(inputArray, outputArray, data):\n",
    "    mp = data['num_char_map_2']\n",
    "    for row1, row2 in zip(inputArray, outputArray):\n",
    "        t1 = ''.join([mp[e1.item()] for e1 in row1])\n",
    "        t2 = ''.join([mp[e2.item()] for e2 in row2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.555227Z",
     "iopub.status.busy": "2025-05-20T16:13:00.554510Z",
     "iopub.status.idle": "2025-05-20T16:13:00.573606Z",
     "shell.execute_reply": "2025-05-20T16:13:00.572995Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.555203Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #Train Model\n",
    "# def train_model():\n",
    "#     # Initialize wandb run first\n",
    "#     with wandb.init(project='CS23S025-Assignment-3-DL') as run:\n",
    "#         config = wandb.config\n",
    "\n",
    "#         # Dynamically name the run after initialization\n",
    "#         run.name = f\"embedding{config.embSize}_cellType{config.cellType}_batchSize{config.batchsize}\"\n",
    "#         # Call your training logic\n",
    "#         train(\n",
    "#              embSize=config.embSize,\n",
    "#             encoderLayers=config.encoderLayers,\n",
    "#             decoderLayers=config.decoderLayers,\n",
    "#             hiddenLayerNuerons=config.hiddenLayerNuerons,\n",
    "#             cellType=config.cellType,\n",
    "#             bidirection=config.bidirection,\n",
    "#             dropout=config.dropout,\n",
    "#             epochs=config.epochs,\n",
    "#             batchsize=config.batchsize,\n",
    "#             learningRate=config.learningRate,\n",
    "#             optimizer=config.optimizer,\n",
    "#             tf_ratio=config.tf_ratio\n",
    "#         )\n",
    "\n",
    "# # Define sweep configuration\n",
    "# sweep_config = {\n",
    "#     'method': 'bayes',\n",
    "#     'name': 'BestModel_WithAttntion',\n",
    "#     'metric': {\n",
    "#         'goal': 'maximize',\n",
    "#         'name': 'validation_accuracy',\n",
    "#     },\n",
    "#     'parameters': {\n",
    "#         'embSize': {'values': [256]},\n",
    "#         'encoderLayers': {'values': [1]},\n",
    "#         'decoderLayers': {'values': [15]},\n",
    "#         'hiddenLayerNuerons': {'values': [512]},\n",
    "#         'cellType': {'values': ['GRU']},\n",
    "#         'bidirection': {'values': ['no']},\n",
    "#         'dropout': {'values': [0.1]},\n",
    "#         'epochs': {'values': [1]},\n",
    "#         'batchsize': {'values': [32]},\n",
    "#         'learningRate': {'values': [1e-4]},\n",
    "#         'optimizer': {'values': ['Nadam']},\n",
    "#         'tf_ratio': {'values': [0.7]}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Create the sweep\n",
    "# sweep_id = wandb.sweep(sweep=sweep_config, project='CS23S025-Assignment-3-DL')\n",
    "\n",
    "# # Launch the sweep agent (make sure to update the entity if needed)\n",
    "# wandb.agent(sweep_id=sweep_id,\n",
    "#     function=train_model,\n",
    "#     count=1,  # or however many runs you want\n",
    "#     entity=\"cs23s025-indian-institute-of-technology-madras\",\n",
    "#     project=\"CS23S025-Assignment-3-DL\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.574661Z",
     "iopub.status.busy": "2025-05-20T16:13:00.574418Z",
     "iopub.status.idle": "2025-05-20T16:13:00.842597Z",
     "shell.execute_reply": "2025-05-20T16:13:00.841731Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.574640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['embedding.weight', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.bias_hh_l0', 'fc.weight', 'fc.bias', 'attention.Wa.weight', 'attention.Wa.bias', 'attention.Ua.weight', 'attention.Ua.bias', 'attention.Va.weight', 'attention.Va.bias'])\n"
     ]
    }
   ],
   "source": [
    "char_to_num_target = data['char_num_map_2']\n",
    "num_to_char_target = data['num_char_map_2']\n",
    "char_to_num_source = data['char_num_map']\n",
    "num_to_char_source = data['num_char_map']\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "embSize = 256\n",
    "encoderLayers = 1\n",
    "decoderLayers = 1\n",
    "hiddenLayerNuerons = 512\n",
    "cellType = \"GRU\"\n",
    "bidirection = 'no'\n",
    "dropout = 0.1\n",
    "epochs = 25\n",
    "batchsize = 32\n",
    "learningRate = 0.0001\n",
    "optimizer = 'Nadam'\n",
    "tf_ratio = 0.7\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(\n",
    "    inputDim=data[\"source_len\"],\n",
    "    embSize=embSize,\n",
    "    encoderLayers=encoderLayers,\n",
    "    hiddenLayerNuerons=hiddenLayerNuerons,\n",
    "    cellType=cellType\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    outputDim=data[\"target_len\"],\n",
    "    embSize=embSize,\n",
    "    hiddenLayerNuerons=hiddenLayerNuerons,\n",
    "    decoderLayers=decoderLayers,\n",
    "    cellType=cellType,\n",
    "    dropout_p=dropout\n",
    ").to(device)\n",
    "\n",
    "# Load the best model\n",
    "checkpoint = torch.load(\"/kaggle/input/attention_bestmodel/pytorch/default/1/best_model_attn.pth\")\n",
    "print(checkpoint['decoder_state_dict'].keys())\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "test_dataset = MyDataset(data_test[\"source_charToNum\"], data_test[\"val_charToNum\"])\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "max_target_len = 23  \n",
    "\n",
    "correct = 0\n",
    "total   = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:13:00.843749Z",
     "iopub.status.busy": "2025-05-20T16:13:00.843456Z",
     "iopub.status.idle": "2025-05-20T16:13:00.849401Z",
     "shell.execute_reply": "2025-05-20T16:13:00.848698Z",
     "shell.execute_reply.started": "2025-05-20T16:13:00.843723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src_stoi = data['char_num_map']\n",
    "src_itos = data['num_char_map']\n",
    "tgt_stoi = data['char_num_map_2']\n",
    "tgt_itos = data['num_char_map_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:55:45.169902Z",
     "iopub.status.busy": "2025-05-20T16:55:45.169335Z",
     "iopub.status.idle": "2025-05-20T16:55:45.177658Z",
     "shell.execute_reply": "2025-05-20T16:55:45.176981Z",
     "shell.execute_reply.started": "2025-05-20T16:55:45.169878Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import torch # Assuming your weights might be torch tensors\n",
    "\n",
    "# Helper function to determine text color (black or white) based on background brightness\n",
    "def get_text_color(bg_hex_color):\n",
    "    \"\"\"\n",
    "    Determines if text should be black or white for good contrast against a given background color.\n",
    "    Args:\n",
    "        bg_hex_color (str): Background color in HEX format (e.g., \"#RRGGBB\").\n",
    "    Returns:\n",
    "        str: \"black\" or \"white\".\n",
    "    \"\"\"\n",
    "    bg_hex_color = bg_hex_color.lstrip('#')\n",
    "    r = int(bg_hex_color[0:2], 16)\n",
    "    g = int(bg_hex_color[2:4], 16)\n",
    "    b = int(bg_hex_color[4:6], 16)\n",
    "    # Calculate luminance using the standard formula\n",
    "    luminance = (0.299 * r + 0.587 * g + 0.114 * b) / 255.0\n",
    "    return 'black' if luminance > 0.5 else 'white'\n",
    "\n",
    "def visualize_attention_to_html(word_tokens, attention_weights, cmap_name='YlOrRd'):\n",
    "    \"\"\"\n",
    "    Generates an HTML string to visualize attention weights on a sequence of word tokens.\n",
    "\n",
    "    Args:\n",
    "        word_tokens (list of str): The list of input tokens (e.g., characters of an English word).\n",
    "        attention_weights (list, np.array, or torch.Tensor): \n",
    "            Attention weights for each token. Should be 1D. Values are expected\n",
    "            to be in the [0, 1] range (e.g., output from a softmax).\n",
    "        cmap_name (str): Name of the matplotlib colormap to use (e.g., 'viridis', 'YlOrRd', 'Blues').\n",
    "\n",
    "    Returns:\n",
    "        str: An HTML string for display.\n",
    "    \"\"\"\n",
    "    if len(word_tokens) != len(attention_weights):\n",
    "        raise ValueError(f\"Length of word_tokens ({len(word_tokens)}) and attention_weights ({len(attention_weights)}) must be the same.\")\n",
    "\n",
    "    # Convert to NumPy array if it's a PyTorch tensor\n",
    "    if isinstance(attention_weights, torch.Tensor):\n",
    "        weights = attention_weights.cpu().squeeze().numpy() # Ensure it's 1D and on CPU\n",
    "    else:\n",
    "        weights = np.array(attention_weights).flatten() # Ensure it's 1D\n",
    "\n",
    "    if weights.ndim != 1:\n",
    "        raise ValueError(f\"attention_weights must be 1D, but got {weights.ndim} dimensions.\")\n",
    "    \n",
    "    # Get the chosen colormap\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    \n",
    "    html_parts = []\n",
    "    for token, weight in zip(word_tokens, weights):\n",
    "        # Map weight to color; cmap expects values in [0,1]\n",
    "        # Softmax output is naturally in this range.\n",
    "        # If weights are very concentrated, consider normalizing for better visual distinction:\n",
    "        # normalized_weight = (weight - weights.min()) / (weights.max() - weights.min() + 1e-9) # if needed\n",
    "        # For now, we assume 'weight' is directly usable.\n",
    "        rgba_color = cmap(weight) \n",
    "        \n",
    "        # Convert RGBA (0-1 range) to HEX for HTML background\n",
    "        bg_hex_color = mcolors.rgb2hex(rgba_color[:3]) # Use RGB part, ignore Alpha for background\n",
    "        \n",
    "        # Determine appropriate text color for readability\n",
    "        text_color = get_text_color(bg_hex_color)\n",
    "        \n",
    "        # Escape HTML special characters in token (important if tokens can contain <, >, &)\n",
    "        # For simple characters, this might not be strictly necessary.\n",
    "        safe_token = token.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n",
    "        \n",
    "        html_parts.append(\n",
    "            f'<span style=\"background-color: {bg_hex_color}; color: {text_color}; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">{safe_token}</span>'\n",
    "        )\n",
    "    \n",
    "    return \"\".join(html_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:55:46.286364Z",
     "iopub.status.busy": "2025-05-20T16:55:46.286108Z",
     "iopub.status.idle": "2025-05-20T16:55:46.291188Z",
     "shell.execute_reply": "2025-05-20T16:55:46.290461Z",
     "shell.execute_reply.started": "2025-05-20T16:55:46.286345Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{anguthiyon}}}}}}}}}}}}}}}}}}}'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['source_data'][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ishaara\n",
    "ishita\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T17:11:33.560322Z",
     "iopub.status.busy": "2025-05-20T17:11:33.559727Z",
     "iopub.status.idle": "2025-05-20T17:11:33.565210Z",
     "shell.execute_reply": "2025-05-20T17:11:33.564477Z",
     "shell.execute_reply.started": "2025-05-20T17:11:33.560299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ishat}}'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_word_str = \"ishat\"\n",
    "idx = 431\n",
    "data_test['source_data'][idx][1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T17:11:33.724942Z",
     "iopub.status.busy": "2025-05-20T17:11:33.724417Z",
     "iopub.status.idle": "2025-05-20T17:11:33.741315Z",
     "shell.execute_reply": "2025-05-20T17:11:33.740682Z",
     "shell.execute_reply.started": "2025-05-20T17:11:33.724914Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens: [0, 54, 12, 17, 28, 3]\n",
      "Number of attention steps: 6\n",
      "Final attention weights shape: torch.Size([6, 30])\n"
     ]
    }
   ],
   "source": [
    "input_sequence = data_test['source_charToNum'][idx].to(device)\n",
    "input_sequence = input_sequence.unsqueeze(0)\n",
    "\n",
    "# Initialize encoder hidden state\n",
    "encoder_hidden = encoder.initHidden(1, device)\n",
    "\n",
    "# Encode the input sequence\n",
    "encoder_states, encoder_hidden = encoder(input_sequence, encoder_hidden)\n",
    "\n",
    "# Prepare for decoding\n",
    "decoder_input = torch.tensor([[0]], device=device) # Start with SOS token\n",
    "decoder_hidden = encoder_hidden # Initial decoder hidden state is the final encoder hidden state\n",
    "\n",
    "# List to store attention weights for each decoder step\n",
    "all_attention_weights = []\n",
    "decoded_words = []\n",
    "\n",
    "# Decoding loop\n",
    "for di in range(max_target_len): # Or until EOS token is predicted\n",
    "    # decoder_output: (batch, 1, output_dim)\n",
    "    # decoder_hidden: (h_n, c_n) or h_n\n",
    "    # attn_weights: (batch, 1, seq_len)\n",
    "    decoder_output, decoder_hidden, attn_weights = decoder(\n",
    "        decoder_input, decoder_hidden, encoder_states\n",
    "    )\n",
    "\n",
    "    # Store the attention weights for this time step\n",
    "    # Squeeze to remove the batch dimension (if batch_size=1) and the decoder output token dimension\n",
    "    all_attention_weights.append(attn_weights.squeeze(0)) # Shape will be (1, seq_len) for batch_size=1\n",
    "\n",
    "    # Get the most likely next token\n",
    "    topv, topi = decoder_output.topk(1)\n",
    "    decoder_input = topi.squeeze(2).detach() # Detach from graph\n",
    "\n",
    "    # For demonstration, let's just print the predicted token (you'd convert it back to character)\n",
    "    # In a real scenario, you'd convert topi to the actual character/word\n",
    "    # and check for EOS token\n",
    "    predicted_token = topi.item()\n",
    "    decoded_words.append(predicted_token)\n",
    "\n",
    "    if predicted_token == 3:\n",
    "        break\n",
    "\n",
    "print(f\"Decoded tokens: {decoded_words}\")\n",
    "print(f\"Number of attention steps: {len(all_attention_weights)}\")\n",
    "\n",
    "# Concatenate all attention weights\n",
    "# If all_attention_weights has elements of shape (1, seq_len), stacking them will give (num_decoder_steps, 1, seq_len)\n",
    "# If you want (num_decoder_steps, seq_len), use squeeze(1)\n",
    "final_attention_matrix = torch.cat(all_attention_weights, dim=0)\n",
    "print(f\"Final attention weights shape: {final_attention_matrix.shape}\") # Should be (num_decoded_steps, 1, encoder_seq_len) if batch_size=1\n",
    "# Or (num_decoded_steps, encoder_seq_len) if you squeeze(1) before append\n",
    "\n",
    "# To visualize or analyze:\n",
    "# final_attention_matrix will have a shape like (Length of Hindi word, 1, Length of English word)\n",
    "# You can then visualize this matrix (e.g., using matplotlib's imshow)\n",
    "# where rows correspond to Hindi output characters and columns to English input characters.\n",
    "# The values in the matrix represent the attention strength.\n",
    "tgt = []\n",
    "for d in decoded_words:\n",
    "    tgt.append(data['num_char_map_2'][d])\n",
    "\n",
    "tgt = ''.join(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T17:11:38.074450Z",
     "iopub.status.busy": "2025-05-20T17:11:38.073883Z",
     "iopub.status.idle": "2025-05-20T17:11:38.092065Z",
     "shell.execute_reply": "2025-05-20T17:11:38.091320Z",
     "shell.execute_reply.started": "2025-05-20T17:11:38.074427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention visualization for the character: इ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">i</span><span style=\"background-color: #a23503; color: white; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">s</span><span style=\"background-color: #ffeede; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">h</span><span style=\"background-color: #fff0e1; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">a</span><span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">t</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention visualization for the character: श\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">i</span><span style=\"background-color: #fff1e4; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">s</span><span style=\"background-color: #fdab66; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">h</span><span style=\"background-color: #f87d29; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">a</span><span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">t</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention visualization for the character: ्\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">i</span><span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">s</span><span style=\"background-color: #fff1e4; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">h</span><span style=\"background-color: #fdd8b2; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">a</span><span style=\"background-color: #fdd4aa; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">t</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention visualization for the character: ट\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">i</span><span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">s</span><span style=\"background-color: #fff5eb; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">h</span><span style=\"background-color: #fff3e7; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">a</span><span style=\"background-color: #feeddc; color: black; padding: 3px 2px; margin: 1px; border-radius: 3px; font-family: monospace;\">t</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example Usage:\n",
    "\n",
    "# Assume 'english_word' is your input\n",
    "\n",
    "input_tokens = list(english_word_str) # ['m', 'o', 'd', 'e', 'l']\n",
    "\n",
    "source_word_tokens = english_word_str\n",
    "num_decoding_steps = 4 # Imagine we are generating 4 target characters\n",
    "\n",
    "# print(f\"\\n--- Simulating attention visualization during decoding for '{''.join(source_word_tokens)}' ---\")\n",
    "for t_step in range(1, max_target_len-2):\n",
    "    # In a real scenario, these weights would come from your decoder at each step\n",
    "    # Generating random weights for demonstration\n",
    "    # simulated_weights = torch.softmax(torch.rand(len(source_word_tokens)), dim=0)\n",
    "    attn_weights = final_attention_matrix[t_step][:len(source_word_tokens)].detach()\n",
    "    html_step_output = visualize_attention_to_html(source_word_tokens, attn_weights, cmap_name='Oranges')\n",
    "    if tgt[t_step] == '}':\n",
    "        break\n",
    "    print(f\"Attention visualization for the character: {tgt[t_step]}\")\n",
    "    display(HTML(html_step_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T17:07:35.892962Z",
     "iopub.status.busy": "2025-05-20T17:07:35.892675Z",
     "iopub.status.idle": "2025-05-20T17:07:35.899457Z",
     "shell.execute_reply": "2025-05-20T17:07:35.898933Z",
     "shell.execute_reply.started": "2025-05-20T17:07:35.892943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7762e-05, 2.8499e-04, 9.0696e-04, 4.9548e-02, 2.8051e-01, 4.0142e-02,\n",
       "        3.7268e-01, 1.5843e-01, 4.7167e-02, 1.1484e-02, 3.8858e-03, 2.2402e-03,\n",
       "        1.8458e-03, 1.7505e-03, 1.7393e-03, 1.7517e-03, 1.7686e-03, 1.7844e-03,\n",
       "        1.7979e-03, 1.8092e-03, 1.8187e-03, 1.8267e-03, 1.8336e-03, 1.8395e-03,\n",
       "        1.8446e-03, 1.8491e-03, 1.8530e-03, 1.8564e-03, 1.8593e-03, 1.8619e-03],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attention_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-20T16:13:01.033796Z",
     "iopub.status.idle": "2025-05-20T16:13:01.034111Z",
     "shell.execute_reply": "2025-05-20T16:13:01.034010Z",
     "shell.execute_reply.started": "2025-05-20T16:13:01.033994Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def transliterate(word: str,\n",
    "                  encoder: nn.Module,\n",
    "                  decoder: nn.Module,\n",
    "                  src_stoi,\n",
    "                  src_itos,\n",
    "                  tgt_stoi,\n",
    "                  tgt_itos,\n",
    "                  device: torch.device,\n",
    "                  max_len: int = 30) -> str:\n",
    "    \"\"\"\n",
    "    Greedy transliteration of `word` (a string of source chars) \n",
    "    into target script characters.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Source → indices (no <sos>/<eos> on encoder side)\n",
    "    # src_idxs = [src_stoi[ch] for ch in word]\n",
    "    # src_tensor = torch.LongTensor(src_idxs).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "    # 2) Run through encoder\n",
    "    encoder_hidden = encoder.initHidden(batch_size=1, device=device)             # hidden init\n",
    "    encoder_states, encoder_hidden = encoder(word, encoder_hidden)\n",
    "    # encoder_states: (seq_len, num_layers, 1, hidden_size)\n",
    "    # encoder_hidden: final hidden state(s)\n",
    "\n",
    "    # 3) Prepare decoder input (start with <sos>)\n",
    "    sos_idx = tgt_stoi['{']\n",
    "    eos_idx = tgt_stoi['}']\n",
    "    dec_input = torch.LongTensor([[sos_idx]]).to(device)         # (1, 1)\n",
    "    dec_hidden = encoder_hidden                                  # seed decoder\n",
    "    \n",
    "    # 4) Greedy decode loop\n",
    "    output_chars = []\n",
    "    for _ in range(max_len):\n",
    "        # decoder returns log-probs over outputDim\n",
    "        dec_out, dec_hidden, attn_weights = decoder(dec_input, dec_hidden, encoder_states)\n",
    "        # dec_out: (1, 1, outputDim)\n",
    "        \n",
    "        # pick highest-prob token\n",
    "        top1 = dec_out.argmax(2)         # (1,1)\n",
    "        token_idx = top1.item()\n",
    "        if token_idx == eos_idx:\n",
    "            break\n",
    "        output_chars.append(tgt_itos[token_idx])\n",
    "        \n",
    "        # next input is current prediction\n",
    "        dec_input = top1\n",
    "\n",
    "    return ''.join(output_chars)\n",
    "\n",
    "# Example usage:\n",
    "encoder.to(device); decoder.to(device)\n",
    "word = data['source_charToNum'][2].unsqueeze(0)\n",
    "print(transliterate(word, encoder, decoder, src_stoi, src_itos, tgt_stoi, tgt_itos, device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-20T16:13:01.035167Z",
     "iopub.status.idle": "2025-05-20T16:13:01.035463Z",
     "shell.execute_reply": "2025-05-20T16:13:01.035328Z",
     "shell.execute_reply.started": "2025-05-20T16:13:01.035314Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#New\n",
    "def get_connectivity_fn(encoder, decoder, word, tgt_stoi, device, max_len = 30):\n",
    "\n",
    "    # encoder.eval() # Set encoder to evaluation mode\n",
    "    # decoder.eval() # Set decoder to evaluation mode\n",
    "\n",
    "    gradient_list = []\n",
    "    batch_size = 1 \n",
    "\n",
    "    encoder_hidden = encoder.initHidden(batch_size, device)\n",
    "    if isinstance(encoder_hidden, tuple): # LSTM\n",
    "        encoder_hidden = (encoder_hidden[0].to(device), enc_state_init[1].to(device))\n",
    "    else: # GRU/RNN\n",
    "        encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "    # Embed encoder input\n",
    "    # Shape: (1, max_input_len, emb_dim)\n",
    "    embedded_in = encoder.embedding(word)\n",
    "    embedded_in.requires_grad_(True) # Crucial for gradient calculation\n",
    "\n",
    "    # Get encoder outputs\n",
    "    # enc_out shape: (1, max_input_len, enc_hidden_size)\n",
    "    # enc_state_final: final encoder hidden state (tuple for LSTM)\n",
    "    outputs, encoder_hidden = encoder.rnn(embedded_in, encoder_hidden)\n",
    "\n",
    "    # prepare encoder states for attention\n",
    "    # convert to (seq_len, batch, hidden*directions)\n",
    "    encoder_states = outputs.permute(1, 0, 2)     # (seq_len, batch, hidden*directions)\n",
    "    # --- Decoder Pass (Iterative) ---\n",
    "    sos_idx = tgt_stoi['{']\n",
    "    eos_idx = tgt_stoi['}']\n",
    "    dec_input = torch.LongTensor([[sos_idx]]).to(device)         # (1, 1)\n",
    "    dec_hidden = encoder_hidden                                  # seed decoder\n",
    "\n",
    "    # 4) Greedy decode loop\n",
    "    output_chars = []\n",
    "    for s in range(max_len):\n",
    "        print(f'This is {s}th iteration')\n",
    "        # decoder returns log-probs over outputDim\n",
    "        # dec_out, dec_hidden, attn_weights = decoder(dec_input, dec_hidden, encoder_states)\n",
    "        # dec_out: (1, 1, outputDim)\n",
    "        # get context from attention\n",
    "        if isinstance(dec_hidden, tuple):  # LSTM\n",
    "            query = dec_hidden[0][-1]  # take last layer's hidden state (batch, hidden)\n",
    "        else:\n",
    "            query = dec_hidden[-1]     # (batch, hidden)\n",
    "        context, attn_weights = decoder.attention(query, encoder_states)\n",
    "\n",
    "        # embed input and concat with context\n",
    "        emb = decoder.embedding(dec_input)              # (batch, 1, emb_size)\n",
    "        emb = F.relu(emb)\n",
    "        rnn_input = torch.cat((emb, context), dim=2)  # (batch, 1, emb_size+hidden)\n",
    "\n",
    "        output, hidden = decoder.rnn(rnn_input, dec_hidden)\n",
    "        bef_output = decoder.dropout(output)                 # (batch, 1, hidden)\n",
    "        dec_out = decoder.softmax(decoder.fc(output))  # (batch, 1, output_dim)\n",
    "        # return prediction, hidden, attn_weights\n",
    "\n",
    "        # pick highest-prob token\n",
    "        top1 = dec_out.argmax(2)         # (1,1)\n",
    "        token_idx = top1.item()\n",
    "        if token_idx == eos_idx:\n",
    "            break\n",
    "        output_chars.append(tgt_itos[token_idx])\n",
    "        \n",
    "        # next input is current prediction\n",
    "        dec_input = top1\n",
    "        \n",
    "        if embedded_in.grad is not None:\n",
    "            embedded_in.grad.zero_() # Zero gradients from previous steps if any accumulate\n",
    "            \n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=bef_output,\n",
    "            inputs=embedded_in,\n",
    "            grad_outputs=torch.ones_like(bef_output),\n",
    "            retain_graph=True, \n",
    "            allow_unused=False # Should be False, if None, something is wrong.\n",
    "        )[0] # We are interested in the gradient w.r.t. embedded_in\n",
    "\n",
    "        if grad is not None:\n",
    "            # Since batch_size is 1, grad will be (1, max_input_len, emb_dim).\n",
    "            # We can store it as is, or select the first element if TF implies that.\n",
    "            # The output of torch.autograd.grad is a tuple, so [0] accesses grad for embedded_in.\n",
    "            gradient_list.append(grad.clone()) # .clone() before .numpy()\n",
    "        else:\n",
    "            # This case should ideally not happen if connections are correct.\n",
    "            print(f\"Warning: Gradient is None at decoding step {t}.\")\n",
    "            gradient_list.append(torch.zeros_like(embedded_in).cpu().numpy())\n",
    "\n",
    "    return ''.join(output_chars), gradient_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-20T16:13:01.037741Z",
     "iopub.status.idle": "2025-05-20T16:13:01.038029Z",
     "shell.execute_reply": "2025-05-20T16:13:01.037920Z",
     "shell.execute_reply.started": "2025-05-20T16:13:01.037909Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import HTML as html_print, display\n",
    "\n",
    "# Optional: define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# HTML formatting helpers\n",
    "def cstr(s, color='black'):\n",
    "    if s == ' ':\n",
    "        return f\"<span style='color:{color}'>{s}</span>\"\n",
    "    else:\n",
    "        return f\"<span style='color:{color}'>{s}</span>\"\n",
    "\n",
    "def print_color(t):\n",
    "    display(html_print(''.join([cstr(ti, color=ci) for ti, ci in t])))\n",
    "\n",
    "def get_clr(value):\n",
    "    colors = [\n",
    "        '#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n",
    "        '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
    "        '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
    "        '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e'\n",
    "    ]\n",
    "    idx = int(value * (len(colors) - 1))\n",
    "    return colors[min(idx, len(colors) - 1)]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x.detach().cpu().numpy()\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Main function to compute scaled gradient norms\n",
    "def get_gradient_norms(grad_list, word, activation=\"sigmoid\"):\n",
    "    grad_norms = []\n",
    "    for grad_tensor in grad_list:\n",
    "        grad_tensor = grad_tensor.detach().cpu()\n",
    "        grad_mags = torch.norm(grad_tensor, dim=1)  # Shape: (seq_len,)\n",
    "        grad_mags = grad_mags[:len(word)].numpy()\n",
    "\n",
    "        if activation == \"softmax\":\n",
    "            grad_scaled = softmax(grad_mags)\n",
    "        elif activation == \"scaler\":\n",
    "            scaler = MinMaxScaler()\n",
    "            grad_scaled = scaler.fit_transform(grad_mags.reshape(-1, 1)).flatten()\n",
    "        else:  # Default: sigmoid\n",
    "            grad_scaled = sigmoid(grad_mags)\n",
    "        grad_norms.append(grad_scaled)\n",
    "    return grad_norms\n",
    "\n",
    "def visualize(grad_norms, word, translated_word):\n",
    "    print(\"Original Word:\", word)\n",
    "    print(\"Transliterated Word:\", translated_word)\n",
    "    for i in range(len(translated_word)):\n",
    "        print(\"Connectivity Visualization for\", translated_word[i], \":\")\n",
    "        text_colours = [(word[j], get_clr(grad_norms[i][j])) for j in range(len(grad_norms[i]))]\n",
    "        print_color(text_colours)\n",
    "\n",
    "# Wrapper that integrates everything\n",
    "def visualise_connectivity(encoder, decoder, word, tgt_stoi, get_connectivity_fn, device, activation=\"sigmoid\"):\n",
    "    translated_word, grad_list = get_connectivity_fn(encoder, decoder, word, tgt_stoi, device)\n",
    "    grad_norms = get_gradient_norms(grad_list, word, activation)\n",
    "    visualize(grad_norms, word, translated_word)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "word = data['source_charToNum'][2].unsqueeze(0)\n",
    "# visualise_connectivity(encoder, decoder, word,tgt_stoi, get_connectivity_fn, device, activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-20T16:13:01.038867Z",
     "iopub.status.idle": "2025-05-20T16:13:01.039077Z",
     "shell.execute_reply": "2025-05-20T16:13:01.038991Z",
     "shell.execute_reply.started": "2025-05-20T16:13:01.038982Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import editdistance\n",
    "import csv\n",
    "\n",
    "char_to_num_target = data['char_num_map_2']\n",
    "num_to_char_target = data['num_char_map_2']\n",
    "char_to_num_source = data['char_num_map']\n",
    "num_to_char_source = data['num_char_map']\n",
    "\n",
    "# Define model parameters\n",
    "embSize = 32\n",
    "encoderLayers = 3\n",
    "decoderLayers = 3\n",
    "hiddenLayerNuerons = 512\n",
    "cellType = \"GRU\"\n",
    "bidirection = 'no'\n",
    "dropout = 0.3\n",
    "epochs = 15\n",
    "batchsize = 64\n",
    "learningRate = 0.001\n",
    "optimizer = 'Nadam'\n",
    "tf_ratio = 1.0\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(\n",
    "    inputDim=data[\"source_len\"],\n",
    "    embSize=embSize,\n",
    "    encoderLayers=encoderLayers,\n",
    "    hiddenLayerNuerons=hiddenLayerNuerons,\n",
    "    cellType=cellType,\n",
    "    bidirection=bidirection\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    outputDim=data[\"target_len\"],\n",
    "    embSize=embSize,\n",
    "    hiddenLayerNuerons=hiddenLayerNuerons,\n",
    "    decoderLayers=decoderLayers,\n",
    "    cellType=cellType,\n",
    "    dropout_p=dropout\n",
    ").to(device)\n",
    "\n",
    "# Load the best model\n",
    "checkpoint = torch.load(\"best_model.pth\")\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "test_dataset = MyDataset(data_test[\"source_charToNum\"], data_test[\"val_charToNum\"])\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "max_target_len = 23  \n",
    "\n",
    "total = 0\n",
    "print_limit = 10\n",
    "\n",
    "def clean(seq):\n",
    "    return ''.join(c for c in seq if c not in ('{', '}'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for source_tensor, target_tensor in test_loader:\n",
    "        source_tensor = source_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "\n",
    "        # Encoder\n",
    "        enc_hidden = encoder.initHidden(batch_size=1)\n",
    "        encoder_outputs, enc_hidden = encoder(source_tensor, enc_hidden)\n",
    "\n",
    "        # Decoder init\n",
    "        dec_hidden = init_decoder_state(enc_hidden, encoderLayers, decoderLayers, cellType)\n",
    "        decoder_input = torch.tensor([[char_to_num_target[\"{\"]]], device=device)\n",
    "\n",
    "        decoded_output = []\n",
    "\n",
    "        for _ in range(max_target_len):\n",
    "            decoder_output, dec_hidden = decoder(decoder_input, dec_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            next_index = topi.view(-1).item()\n",
    "            next_char = num_to_char_target[next_index]\n",
    "\n",
    "            if next_char == \"}\":\n",
    "                break\n",
    "\n",
    "            decoded_output.append(next_char)\n",
    "            decoder_input = torch.tensor([[next_index]], device=device)\n",
    "\n",
    "        # Get input/output strings, remove {} padding\n",
    "        input_seq = clean([num_to_char_source[i.item()] for i in source_tensor[0]])\n",
    "        target_seq = clean([num_to_char_target[i.item()] for i in target_tensor[0]])\n",
    "        predicted_seq = clean(decoded_output)\n",
    "\n",
    "        if predicted_seq == target_seq:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        if total <= print_limit:\n",
    "            print(f\"Input:     {input_seq}\")\n",
    "            print(f\"Target:    {target_seq}\")\n",
    "            print(f\"Predicted: {predicted_seq}\\n\")\n",
    "\n",
    "        if total <= 3 or (total <= 20 and predicted_seq == target_seq):\n",
    "            print(f\"MATCH! Input: {input_seq} | Target: {target_seq} | Predicted: {predicted_seq}\")\n",
    "        elif total <= 20:\n",
    "            print(f\"DIFF!  Input: {input_seq} | Target: {target_seq} | Predicted: {predicted_seq}\")\n",
    "\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7419837,
     "sourceId": 11813384,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 351070,
     "modelInstanceId": 330220,
     "sourceId": 403951,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
